{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw-U_6NhfHOs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dense,GRU, LSTM, Bidirectional, Embedding, Dropout, Flatten\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC3VFVQngnA0"
      },
      "outputs": [],
      "source": [
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJTENDo9iAwD"
      },
      "outputs": [],
      "source": [
        "  def load_dataset(filename):\n",
        "     df= pd.read_csv(filename,encoding = \"utf-8\", names = [\"Sentence\", \"Intent\"])\n",
        "     intent =df[\"Intent\"]\n",
        "     unique_intent = list(set(intent))\n",
        "     sentences = list(df['Sentence'])\n",
        "     return (intent,unique_intent,sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubP1foXqiXHl"
      },
      "outputs": [],
      "source": [
        "  intent,unique_intent,sentences = load_dataset('/content/dataCorp.csv')\n",
        "\n",
        "  print(sentences[:11])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx_UkP6ruSVQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lkDrlLuu-eU"
      },
      "outputs": [],
      "source": [
        "#  stemmer= LancasterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phz_KwmH2vlL"
      },
      "outputs": [],
      "source": [
        " df= pd.read_csv(\"/content/stopwords.csv\",encoding = \"utf-8\", names = [\"stopwords\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj-HV7b8923R"
      },
      "outputs": [],
      "source": [
        "words =df[\"stopwords\"]\n",
        "stopwords = list(set(words))\n",
        "print(stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzKyYe0YzAQz"
      },
      "outputs": [],
      "source": [
        "  def cleaning(sentences):\n",
        "       words = []\n",
        "       for s in sentences:\n",
        "         clean= re.sub(r'[^ a-z A-Z 0-9]',\" \",s)\n",
        "         text_tokens = clean.split()\n",
        "         w =[word for word in text_tokens if not word in stopwords]\n",
        "         words.append([i.lower() for i in w])\n",
        "\n",
        "       return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgYCTGJO0Ws_"
      },
      "outputs": [],
      "source": [
        "  cleaned_words = cleaning(sentences)\n",
        "  print(len(cleaned_words))\n",
        "  print(cleaned_words)\n",
        "  print(unique_intent)\n",
        "  print(intent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE9UE-4VYH4w"
      },
      "outputs": [],
      "source": [
        "  def ready(sentences):\n",
        "    print(sentences)\n",
        "    # Tokenizer API\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    print(tokenizer.word_index)\n",
        "    total_words = len(tokenizer.word_index)+1\n",
        "    input_sequences = []\n",
        "    for s in sentences:\n",
        "      token_list= tokenizer.texts_to_sequences([s])[0]\n",
        "      print(token_list)\n",
        "      for i in range(0,len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "    y = []                 # for intent array\n",
        "    for i in range(len(intent)):\n",
        "      temp = len(sentences[i].split(','))\n",
        "    #  print(temp)\n",
        "     # print(sentences[i],temp)\n",
        "    print(temp)\n",
        "    for j in range(temp):\n",
        "        y.append(intent[i])\n",
        "    return input_sequences,y,total_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r1vZzAhcj5f"
      },
      "outputs": [],
      "source": [
        "   input_sequences,y,total_words = ready(sentences)\n",
        "   print(input_sequences)    #n-gram result\n",
        "   #print(y)\n",
        "   #print(total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9msnIeyj89Q"
      },
      "outputs": [],
      "source": [
        "  max_sequence_len = max([len(x) for x in input_sequences])\n",
        "  input_sequences = np.array(pad_sequences(input_sequences,maxlen=max_sequence_len,padding='post'))\n",
        "  print(input_sequences)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FQ9iu2XkWaK"
      },
      "outputs": [],
      "source": [
        "  def create_tokenizer(words,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_^{|}~'):\n",
        "        token = Tokenizer(filters=filters)\n",
        "        token.fit_on_texts(words)\n",
        "        return token\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCVrOkFoky9q"
      },
      "outputs": [],
      "source": [
        "  def max_length ( words ) :\n",
        "    return(len(max(words,key=len)))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fS0Tna7lMnK",
        "outputId": "c467861a-d45c-49a7-8e8e-35abe1631a89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab Size = 185 and Maximum length = 8\n"
          ]
        }
      ],
      "source": [
        "  word_tokenizer = create_tokenizer ( cleaned_words )\n",
        "  vocab_size = len(word_tokenizer.word_index)+1\n",
        "  max_length = max_length(cleaned_words)\n",
        "  print(\"Vocab Size = %d and Maximum length = %d\"%(vocab_size,max_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y6TD4C_llsN"
      },
      "outputs": [],
      "source": [
        "  word_tokenizer.word_index\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhxjmMx_m41Y"
      },
      "outputs": [],
      "source": [
        "  def encoding_doc(token,words):                              #tokenizer object\n",
        "    return(token.texts_to_sequences(words))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfnyRKAknf2E"
      },
      "outputs": [],
      "source": [
        "encoded_doc = encoding_doc(word_tokenizer,cleaned_words)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upYcQiUgn6V_"
      },
      "outputs": [],
      "source": [
        "  def padding_doc(encoded_doc,max_length):\n",
        "    return(pad_sequences(encoded_doc,maxlen = max_length, padding = \"post\"))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d2vBgY4ofLR"
      },
      "outputs": [],
      "source": [
        "  padded_doc = padding_doc(encoded_doc,max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d-ZOrxhovfZ",
        "outputId": "d2458d70-4389-4977-b401-a4775c3ac7b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 2  6  4 22  0  0  0  0]\n"
          ]
        }
      ],
      "source": [
        "  print(padded_doc[7])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8cSkAU-sspu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CscPxA-1p8js"
      },
      "outputs": [],
      "source": [
        "  print(\"shape of padded docs=\",padded_doc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIgyMQsnvEMl"
      },
      "outputs": [],
      "source": [
        "  #tokenizer with filter changed\n",
        "  output_tokenizer = create_tokenizer(unique_intent, filters='!\"#$%&()*+,-./:;<=>?@[\\]^`{|}~')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZxB691Wz8aS",
        "outputId": "b846c765-f024-42f2-b6ee-4cc237ba2e7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'order': 1, 'menu': 2, 'price': 3, 'end': 4, 'greet': 5}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "  output_tokenizer.word_index\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkV1zSdk0SDt"
      },
      "outputs": [],
      "source": [
        "  encoded_output = encoding_doc(output_tokenizer,intent)\n",
        "  print(encoded_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lomSj6JD0qYu"
      },
      "outputs": [],
      "source": [
        "  encoded_output = np.array(encoded_output).reshape(len(encoded_output),1)\n",
        "  # print(encoded_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWkby2F6mqNL"
      },
      "outputs": [],
      "source": [
        "  def one_hot(encode):\n",
        "   o = OneHotEncoder(sparse=False)\n",
        "   return(o.fit_transform(encode))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vfKlgmpnHW1"
      },
      "outputs": [],
      "source": [
        "  output_one_hot = one_hot(encoded_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpveDL2Nn9Fi",
        "outputId": "f7948a95-effe-4ec4-e11c-db531cad6279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(163, 5)\n"
          ]
        }
      ],
      "source": [
        "  output_one_hot.shape\n",
        "  print(output_one_hot.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15QWkmqsoQo9"
      },
      "outputs": [],
      "source": [
        "  from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWxLKqR2oxyz"
      },
      "outputs": [],
      "source": [
        "  train_X, val_X,train_Y,val_Y = train_test_split(padded_doc, output_one_hot ,shuffle=True,test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tp0ZRXN6pjbG"
      },
      "outputs": [],
      "source": [
        "  print(\"shape of train_X= %s and train_Y = %s\"%( train_X.shape , train_Y.shape))\n",
        "  print (\"Shape of val_X= %s and val_Y = %s\"%( val_X.shape , val_Y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZyZ4sPXqJbs"
      },
      "outputs": [],
      "source": [
        "from keras.layers.pooling import GlobalAveragePooling1D\n",
        "def create_model(vocab_size,max_length):\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(vocab_size,256,input_length = max_length))\n",
        "      # model.add( GRU(128))\n",
        "      model.add ( LSTM ( 64) )\n",
        "      model.add(Flatten())\n",
        "      model.add(Dense(16,activation=\"relu\",))\n",
        "      model.add(Dense( 5, activation = \"softmax\"))\n",
        "      return model\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-Vk9MVxqrJv"
      },
      "outputs": [],
      "source": [
        "  model = create_model(vocab_size,max_length)\n",
        "\n",
        "  model.compile(loss = \"categorical_crossentropy\",optimizer = \"adam\",metrics = [\"accuracy\"])\n",
        "  model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvJ7vuQ1J6ix"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMGE8H3x0LQV"
      },
      "outputs": [],
      "source": [
        "  filename = 'model.h5'\n",
        "  checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0,\n",
        "    patience=1,\n",
        "    verbose=2,\n",
        "    mode='auto',\n",
        "    baseline=None,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "  hist= model.fit(train_X, train_Y, epochs = 50, batch_size=16, verbose=1, validation_data = (val_X,val_Y), callbacks=[checkpoint, early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0fWIWT2mMWm"
      },
      "outputs": [],
      "source": [
        "  model= load_model(\"model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3l34Hm8J8rY"
      },
      "outputs": [],
      "source": [
        "  def predictions(text):\n",
        "    clean = re.sub(r'[^ a-z A-Z 0-9]',',',text)\n",
        "    test_word = clean.split()\n",
        "    #print(test_word)\n",
        "    test_word = [w.lower() for w in test_word]\n",
        "    test_ls = word_tokenizer.texts_to_sequences(test_word)\n",
        "    #print(test_ls)\n",
        "    if [] in test_ls:\n",
        "      test_ls = list(filter(None,test_ls))      #removing None values from list\n",
        "\n",
        "    test_ls = np.array(test_ls).reshape(1,len(test_ls))      # 1 row\n",
        "    print(test_ls);\n",
        "    x = padding_doc(test_ls,max_length)\n",
        "   # print(x)\n",
        "    pred = model.predict(x)\n",
        "\n",
        "   # print(pred)\n",
        "    return pred\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAjiKozrMG96"
      },
      "outputs": [],
      "source": [
        "    def get_final_output(pred, classes):\n",
        "     print(pred[0])\n",
        "     predictions = pred[0]\n",
        "    # print(predictions)\n",
        "     classes = np.array(classes)\n",
        "    # print(classes)\n",
        "     ids = np.argsort( -predictions)\n",
        "    # print(ids)\n",
        "     classes = classes[ids]\n",
        "    # print(classes[ids])\n",
        "     predictions = -np.sort( -predictions)\n",
        "     \n",
        "     return classes[0]\n",
        "    #  for i in range(pred.shape[1]):\n",
        "    #           print(\"%s has confidence = %s\" %(classes[i], (predictions[i])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTKN7rK7OdHe"
      },
      "outputs": [],
      "source": [
        "    pip install googletrans==3.1.0a0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTzXEgI2PA22"
      },
      "outputs": [],
      "source": [
        "  from googletrans import Translator\n",
        "  translator= Translator()\n",
        "  text= translator.translate('मुझे बिरयानी ऑर्डर करनी है').text\n",
        "  pred = predictions(text)\n",
        "  get_final_output(pred, unique_intent)\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6eoe1H0oj1N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOmufAG9kKmE"
      },
      "outputs": [],
      "source": [
        "DB = [{\n",
        "       'name': 'simple veg',\n",
        "       'quarter': 125,\n",
        "       'half': 205,\n",
        "       'full': 265,\n",
        "       'type': 'veg',\n",
        "      \n",
        "        },\n",
        "       \n",
        "     {\n",
        "        'name':'chicken 65',\n",
        "        'quarter': 125,\n",
        "        'half':205,\n",
        "        'full': 260,\n",
        "        'type':'non-veg',\n",
        "       },\n",
        "    \n",
        "   {\n",
        "    'name': 'fish fry',\n",
        "    'quarter': 160,\n",
        "    'half': 240,\n",
        "    'full': 310,\n",
        "    'type':'non-veg',\n",
        "   },\n",
        "   {\n",
        "     'name':'prawns fry', \n",
        "     'quarter': 140,\n",
        "     'half': 220,\n",
        "     'full': 290,\n",
        "     'type': 'non-veg',},\n",
        "       \n",
        "   {\n",
        "    'name': 'paneer',\n",
        "    'quarter': 210,\n",
        "    'half': 270,\n",
        "    'full': 340,\n",
        "    'type': 'veg',\n",
        "       \n",
        "   },\n",
        "       \n",
        "   {\n",
        "    'name': 'kaju',\n",
        "    'quarter': 210,\n",
        "    'half': 270,\n",
        "    'full': 340,\n",
        "    'type': 'veg',\n",
        "   },\n",
        "\n",
        "  {\n",
        "    'name': 'egg',\n",
        "    'quarter': 170,\n",
        "    'half': 240,\n",
        "    'full': 310,\n",
        "    'type': 'veg',\n",
        "   },\n",
        "       \n",
        "   {\n",
        "   'name':'mushroom',\n",
        "   'quarter': 180,\n",
        "   'half': 250,\n",
        "   'full': 320,\n",
        "   'type': 'veg',\n",
        "   },\n",
        "       \n",
        "   {\n",
        "   'name': 'chicken',\n",
        "   'quarter': 170,\n",
        "   'half': 250,\n",
        "   'full': 310,\n",
        "   'type': 'non-veg',\n",
        "   },\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFrY5GuUmgjD"
      },
      "outputs": [],
      "source": [
        "    pip install word2number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQIt8Rjqpvq5"
      },
      "outputs": [],
      "source": [
        "  lang = 'en' #language of the chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kccCN-Zup1-J"
      },
      "outputs": [],
      "source": [
        "order_list=[]\n",
        "sizes = ['quarter','half','full']\n",
        "briyani = [obj['name'] for obj in DB]\n",
        "     #print ( pizzas ) \n",
        "from word2number import w2n\n",
        "def get_overall_info(sentence):\n",
        "       match = re.search('(?P<quantity>([ 1-9 ]|a|one|two|three|four|five|six|seven|eight|nine))??(?P<size>(quarter half full))??(?P<name>( prawns fry|chicken 65|prawns fry| kaju | paneer | egg | mushroom | chicken roll))? briyani?',sentence)\n",
        "       entity_dict = match.groupdict()\n",
        "       print('Entity Dict before:',entity_dict)\n",
        "       if entity_dict['quantity']=='a':\n",
        "          entity_dict['quantity']=1\n",
        "        # elif entity_dict[ ‘quantity’]= None\n",
        "        # entity_dict[ ‘quantity’ ]=1\n",
        "       elif entity_dict['quantity'] !=None:\n",
        "         entity_dict[ 'quantity' ]=str(w2n.word_to_num(str(entity_dict['quantity'])))\n",
        "         #print(entity_dict)\n",
        "       entity_dict = ask(entity_dict)\n",
        "       #print(entity_dict)\n",
        "       #required pizza object\n",
        "       briyani_obj = {}\n",
        "       for obj in DB:\n",
        "            if obj['name']==entity_dict['name']:\n",
        "              briyani_object = obj\n",
        "       overall_info_object = {'quantity':str(entity_dict['quantity']), 'price':str(briyani_object[entity_dict['size']]),'pobj':briyani_object}\n",
        "       return overall_info_object,entity_dict\n",
        "\n",
        "def ask(entity_dict):\n",
        "       for i in entity_dict.keys():\n",
        "          print(entity_dict)\n",
        "          if entity_dict[i] == None:\n",
        "              if(i=='quantity'):\n",
        "                  while(True):\n",
        "                      user_input = input(translator.translate(\"How many  do you want?\",dest=lang).text)\n",
        "                      user_input_translated = translator.translate(user_input).text.lower()\n",
        "                      if int(user_input_translated)<1 or int(user_input_translated) > 9:\n",
        "                        print(translator.translate('Sorry you cannot order'+str(user_input_translated),dest=lang).text)\n",
        "                      else:\n",
        "                        entity_dict[i] = str(user_input_translated)\n",
        "                        break;\n",
        "                                                   \n",
        "              elif i=='size':\n",
        "                        while(True):\n",
        "                            user_input = input(translator.translate(\"We have quarter, half and full , choose one\", dest=lang).text)\n",
        "                            user_input_translated = translator.translate(user_input).text.lower()\n",
        "                          # print(user_input_translated) #remove later\n",
        "                            if user_input_translated not in sizes:\n",
        "                                print(translator.translate('Please choose the correct size',dest=lang).text)\n",
        "                            else:\n",
        "                              entity_dict[i]=user_input_translated\n",
        "                              break;\n",
        "              elif i=='name':\n",
        "                        while(True):\n",
        "                           user_input = input(translator.translate('what Briyani would you like to have',dest=lang).text)\n",
        "                           user_input_translated = translator.translate(user_input).text.lower()\n",
        "                           if user_input_translated not in briyani:\n",
        "                               print(translator.translate('Please choose the correct Order',dest=lang).text)\n",
        "                           else:\n",
        "                             entity_dict[i]=user_input_translated\n",
        "                             print(entity_dict)\n",
        "                             break\n",
        "\n",
        "       return entity_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt4jP0f8tN0A"
      },
      "outputs": [],
      "source": [
        "def response(sentence, intent):\n",
        "          if intent == 'greet':\n",
        "            response = 'Hi! How may I help you?'\n",
        "            return response\n",
        "          elif intent == 'menu':\n",
        "              for obj in DB:\n",
        "                response=str(obj['name'])+ 'price:' +str(obj['full'])+'rs'\n",
        "                print(translator.translate(response,dest=lang).text)\n",
        "          elif intent=='order':\n",
        "                  overall_info_object,entity_dict = get_overall_info(sentence)\n",
        "                  order_list.append(overall_info_object)\n",
        "                            #print(order_list)\n",
        "                  response='Thank You for your order of ' +str(order_list[-1]['quantity'])+' '+str(entity_dict['name']+' '+ 'briyani')\n",
        "                  return response\n",
        "          elif intent == 'price':\n",
        "                     overall_info_object,entity_dict = get_overall_info(sentence)\n",
        "                     price = int(overall_info_object['quantity'])*int(overall_info_object['price'])\n",
        "                     response='The price is Rs.'+str(price)\n",
        "                     return response\n",
        "\n",
        "          elif intent == 'end':\n",
        "                  bill =0\n",
        "                  for i in order_list:\n",
        "                      bill += int(i['quantity'])*int(i['price'])\n",
        "                  if(bill):\n",
        "                        response=' Thank You for Orderingin  Rezzo restaurent ,Your total bill amount is Rs.'+str(bill)\n",
        "                        return response\n",
        "                  else:\n",
        "                        response='see you later'\n",
        "                        return response\n",
        "                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-anS5y7xysHt"
      },
      "outputs": [],
      "source": [
        "\n",
        "while(1):\n",
        "    raw_input = input()\n",
        "    translated_input = translator.translate(raw_input).text.lower()\n",
        "    if(translated_input=='bill please'):\n",
        "      intent='end';\n",
        "    else:\n",
        "       pred = predictions(translated_input)\n",
        "       intent = get_final_output(pred,unique_intent)\n",
        "    print(intent)\n",
        "    if(intent=='end'): \n",
        "        resp = response(translated_input, intent)\n",
        "        print(translator.translate(resp,dest=lang).text)\n",
        "        break\n",
        "    else:\n",
        "        resp =response(translated_input, intent)\n",
        "        print(translator.translate(resp,dest=lang).text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}